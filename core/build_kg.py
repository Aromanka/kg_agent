import os
import glob
import json
import re
import time
import datetime
import pandas as pd
from openai import OpenAI
from tqdm import tqdm
from ..agents.diet.prompts import (
    DIET_KG_EXTRACT_SCHEMA_PROMPT as SCHEMA_PROMPT,
    DIET_VALID_RELS
)

# å¤„ç† PDF å’Œ Word çš„åº“
import pymupdf4llm
from docx import Document

# ================= é…ç½®åŠ è½½ =================
CONFIG_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "config.json")
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = json.load(f)

DEEPSEEK_API_KEY = config["deepseek"]["api_key"]
DEEPSEEK_BASE_URL = config["deepseek"]["base_url"]
MODEL_NAME = config["deepseek"]["model"]

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================
# 1. å¾…å¤„ç†æ–‡çŒ®è·¯å¾„
INPUT_DIR = "data"

# 2. ç»“æœä¿å­˜çš„åŸºç¡€ç›®å½• (æ‰€æœ‰å†å²è®°å½•éƒ½ä¼šå­˜åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹)
OUTPUT_BASE_DIR = "output_history"

# 4. æ–‡æœ¬åˆ‡åˆ†è®¾ç½®
CHUNK_SIZE = 1000  
OVERLAP = 200      

# ===============================================
def read_excel(file_path):
    """
    ã€æ–°å¢ã€‘è¯»å– Excel å¹¶å°†æ¯ä¸€è¡Œè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€å¥å­
    """
    print(f"ğŸ“Š æ­£åœ¨è§£æ Excel: {os.path.basename(file_path)}")
    text_content = []
    try:
        # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨ (sheet_name=None è¿”å›å­—å…¸)
        dfs = pd.read_excel(file_path, sheet_name=None, engine='openpyxl')
        
        for sheet_name, df in dfs.items():
            if df.empty: continue
            
            # 1. æ¸…æ´—è¡¨å¤´ (è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œå»ç©ºæ ¼)
            headers = [str(col).strip().replace("\n", "") for col in df.columns]
            
            # 2. éå†æ¯ä¸€è¡Œ
            # fillna('') é˜²æ­¢ç©ºå€¼æŠ¥é”™
            for _, row in df.fillna('').iterrows():
                row_parts = []
                for header, cell_value in zip(headers, row):
                    # å¦‚æœå•å…ƒæ ¼ä¸ä¸ºç©ºï¼Œå°±æ‹¼æ¥ "è¡¨å¤´æ˜¯æ•°å€¼"
                    val_str = str(cell_value).strip().replace("\n", " ")
                    if val_str and val_str.lower() != 'nan':
                        row_parts.append(f"{header}æ˜¯{val_str}")
                
                # 3. ç»„åˆæˆå¥å­
                if row_parts:
                    # ä¾‹: "åœ¨è¡¨æ ¼Sheet1ä¸­ï¼Œè¯ç‰©æ˜¯äºŒç”²åŒèƒï¼Œå‰‚é‡æ˜¯500mgã€‚"
                    sentence = f"åœ¨æ•°æ®è¡¨{sheet_name}ä¸­ï¼Œ" + "ï¼Œ".join(row_parts) + "ã€‚"
                    text_content.append(sentence)
                    
        return "\n".join(text_content)

    except Exception as e:
        print(f"âš ï¸ Excel è¯»å–å¤±è´¥ {file_path}: {e}")
        return ""
def read_docx(file_path):
    """ æå– Wordï¼Œå«è¡¨æ ¼è½¬è‡ªç„¶è¯­è¨€é€»è¾‘ """
    try:
        doc = Document(file_path)
        text_content = []
        for para in doc.paragraphs:
            if para.text.strip():
                text_content.append(para.text)
        if doc.tables:
            for table in doc.tables:
                headers = [cell.text.strip().replace("\n", "") for cell in table.rows[0].cells]
                for row in table.rows[1:]:
                    row_cells = [cell.text.strip().replace("\n", " ") for cell in row.cells]
                    row_parts = []
                    for i in range(len(row_cells)):
                        if i < len(headers) and row_cells[i]:
                            row_parts.append(f"{headers[i]}æ˜¯{row_cells[i]}")
                    if row_parts:
                        text_content.append("ï¼Œ".join(row_parts) + "ã€‚")
        return "\n".join(text_content)
    except Exception as e:
        print(f"âš ï¸ Word è¯»å–å¤±è´¥ {file_path}: {e}")
        return ""

def read_pdf(file_path):
    """ æå– PDF (pymupdf4llm) """
    try:
        return pymupdf4llm.to_markdown(file_path)
    except Exception as e:
        print(f"âš ï¸ PDF è¯»å–å¤±è´¥ {file_path}: {e}")
        return ""

def read_txt(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except:
        return ""

def clean_text(text):
    """Clean text by removing citations, page numbers, and other noise."""
    # Remove source citations (e.g., [1], [2,3])
    text = re.sub(r'\[\d+(?:,\s*\d+)*\]', '', text)
    # Remove page numbers (isolated numbers on their own line)
    text = re.sub(r'\n\s*\d+\s*\n', '\n', text)
    # Remove multiple consecutive empty lines
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def split_text_by_headers(text, chunk_size=CHUNK_SIZE):
    """Split text by Markdown headers (##) to keep sections together."""
    if not text: return []

    # Split by Markdown headers (##)
    sections = re.split(r'(^##\s+.*)', text, flags=re.MULTILINE)

    chunks = []
    current_chunk = ""

    for part in sections:
        if not part: continue

        # If adding this part exceeds limit, save current chunk
        if len(current_chunk) + len(part) > chunk_size:
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = part
        else:
            current_chunk += part

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

def split_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):
    """Legacy fallback: simple chunking by character limit."""
    if not text: return []
    chunks = []
    start = 0
    length = len(text)
    while start < length:
        end = min(start + chunk_size, length)
        if end < length:
            next_newline = text.find('\n', end, end + 100)
            if next_newline != -1:
                end = next_newline
        chunk = text[start:end]
        if len(chunk.strip()) > 20:
            chunks.append(chunk)
        start += (chunk_size - overlap)
    return chunks

def extract_triplets_with_deepseek(client, text_chunk):
    """
    Extract triplets using DeepSeek with JSON Object response format.
    Prioritizes "triplets" key from the response.
    """
    if len(text_chunk.strip()) < 10: return []

    prompt = f"{SCHEMA_PROMPT}\n\n## å¾…å¤„ç†æ–‡æœ¬\n{text_chunk}"

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are a helpful medical assistant. Always output valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            stream=False,
            response_format={'type': 'json_object'}
        )
        content = response.choices[0].message.content.strip()

        try:
            data = json.loads(content)

            # Priority 1: Look for "triplets" key (required by new prompt)
            if isinstance(data, dict):
                if "triplets" in data and isinstance(data["triplets"], list):
                    return data["triplets"]

                # Priority 2: Look for any list value as fallback
                for val in data.values():
                    if isinstance(val, list):
                        return val

            # Priority 3: Direct list response
            elif isinstance(data, list):
                return data

            return []

        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}, å†…å®¹ç‰‡æ®µ: {content[:100]}...")
            return []

    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")
        time.sleep(2)
        return []

def main():
    # 1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶å¤¹
    if not os.path.exists(INPUT_DIR):
        os.makedirs(INPUT_DIR)
        print(f"è¯·åˆ›å»º {INPUT_DIR} å¹¶æ”¾å…¥æ–‡ä»¶")
        return

    # 2. ç”Ÿæˆæœ¬æ¬¡è¿è¡Œçš„è¾“å‡ºæ–‡ä»¶å¤¹ (æ ¼å¼: Output_History/Run_20231223_143005)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    current_output_dir = os.path.join(OUTPUT_BASE_DIR, f"Run_{timestamp}")

    # åˆ›å»ºæ–‡ä»¶å¤¹
    os.makedirs(current_output_dir, exist_ok=True)
    print(f"ğŸ“‚ æœ¬æ¬¡ç»“æœå°†ä¿å­˜åœ¨: {current_output_dir}")

    # 3. åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)

    # 4. æ‰«ææ–‡ä»¶
    files = glob.glob(os.path.join(INPUT_DIR, "*.pdf")) + \
        glob.glob(os.path.join(INPUT_DIR, "*.docx")) + \
        glob.glob(os.path.join(INPUT_DIR, "*.txt")) + \
        glob.glob(os.path.join(INPUT_DIR, "*.xlsx"))

    if not files: 
        print(f"âš ï¸ '{INPUT_DIR}' æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ã€‚")
        return

    print(f"ğŸ” å‘ç° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹æå–...")

    all_triplets = []
    seen_hashes = set()
    processed_files_log = [] # è®°å½•å¤„ç†äº†å“ªäº›æ–‡ä»¶
    start_time = time.time()

    valid_rels = DIET_VALID_RELS

    # 5. å¾ªç¯å¤„ç†
    for file_path in tqdm(files, desc="æ€»è¿›åº¦"):
        file_name = os.path.basename(file_path)
        processed_files_log.append(file_name)

        ext = file_path.lower()
        if ext.endswith(".pdf"): content = read_pdf(file_path)
        elif ext.endswith(".docx"): content = read_docx(file_path)
        elif ext.endswith(".xlsx"): content = read_excel(file_path)
        else: content = read_txt(file_path)

        if not content: continue

        # Clean text and split by headers
        cleaned_content = clean_text(content)
        chunks = split_text_by_headers(cleaned_content)

        for chunk in tqdm(chunks, desc=f"è§£æ {file_name[:10]}", leave=False):
            triplets = extract_triplets_with_deepseek(client, chunk)

            for t in triplets:
                if "head" in t and "relation" in t and "tail" in t:
                    if t['relation'] in valid_rels:
                        t_hash = f"{t['head']}_{t['relation']}_{t['tail']}"
                        if t_hash not in seen_hashes:
                            seen_hashes.add(t_hash)
                            t["source"] = file_name
                            all_triplets.append(t)

                time.sleep(0.1)

    # 6. ä¿å­˜ç»“æœåˆ°æ–°åˆ›å»ºçš„æ–‡ä»¶å¤¹
    duration = time.time() - start_time

    # å®šä¹‰æ–°è·¯å¾„
    output_json_path = os.path.join(current_output_dir, "kg_triplets.json")
    output_csv_path = os.path.join(current_output_dir, "kg_triplets.csv")
    log_path = os.path.join(current_output_dir, "process_log.txt")

    print("-" * 40)
    print(f"âœ… æå–å®Œæˆï¼è€—æ—¶: {duration:.2f}ç§’")
    print(f"ğŸ•¸ï¸  å…±è·å¾— {len(all_triplets)} ä¸ªå”¯ä¸€ä¸‰å…ƒç»„ã€‚")

    # ä¿å­˜ JSON
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(all_triplets, f, indent=4, ensure_ascii=False)

    # ä¿å­˜ CSV
    df = pd.DataFrame(all_triplets)
    if not df.empty:
        cols = ["head", "relation", "tail", "source"]
        existing = [c for c in cols if c in df.columns]
        df = df[existing]
        df.to_csv(output_csv_path, index=False, encoding='utf_8_sig')

    # ä¿å­˜æ—¥å¿— (æ–¹ä¾¿ä½ ä»¥åçŸ¥é“è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œæ˜¯å“ªäº›æ•°æ®è·‘å‡ºæ¥çš„)
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(f"è¿è¡Œæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"è€—æ—¶: {duration:.2f} ç§’\n")
        f.write(f"æå–ä¸‰å…ƒç»„æ•°é‡: {len(all_triplets)}\n")
        f.write("\nå¤„ç†çš„æ–‡ä»¶åˆ—è¡¨:\n")
        for fname in processed_files_log:
            f.write(f"- {fname}\n")

    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³æ–‡ä»¶å¤¹: {current_output_dir}")

if __name__ == "__main__":
    main()