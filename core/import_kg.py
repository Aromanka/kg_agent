"""
Import knowledge graph data from JSON/CSV files into Neo4j database.
Run after build_kg_deepseek.py has generated the output files.
"""
import os
import json
import glob
from neo4j import GraphDatabase
from config_loader import NEO4J_URI, NEO4J_AUTH

# ================= é…ç½®åŠ è½½ =================

# æ•°æ®æ¥æºç›®å½• (build_kg_deepseek.py çš„è¾“å‡ºç›®å½•)
OUTPUT_HISTORY_DIR = "output_history"
INPUT_DIR = "data"  # ä¹Ÿæ”¯æŒç›´æ¥å¯¼å…¥åŸå§‹æ–‡ä»¶

# ================= å®ä½“æ ‡ç­¾æ˜ å°„ =================

# æ ¹æ®å…³ç³»ç±»å‹ç¡®å®šå®ä½“æ ‡ç­¾
# æ ¼å¼: {relation_type: (head_label, tail_label)}
RELATION_LABEL_MAP = {
    # Disease ç›¸å…³: tail æ˜¯ Disease
    "Diet_Disease": ("Food", "Disease"),
    "Food_Disease": ("Food", "Disease"),
    "Nutrient_Disease": ("Nutrient", "Disease"),
    "Restriction_Disease": ("Restriction", "Disease"),
    "Contraindication_Food": ("Food", "Disease"),
    "Interaction_Food": ("Food", "Disease"),

    # Diet ç›¸å…³: tail æ˜¯ Diet
    "Food_Diet": ("Food", "Diet"),

    # Food å±æ€§: tail æ˜¯ Food
    "Amount_Food": ("Amount", "Food"),
    "Frequency_Food": ("Frequency", "Food"),
    "Method_Food": ("Method", "Food"),

    # Food ç›Šå¤„/é£é™©: tail æ˜¯ Benefit/Risk
    "Benefit_Food": ("Food", "Benefit"),
    "Risk_Food": ("Food", "Risk"),
}


def infer_entity_label(name: str, position: str, relation: str) -> str:
    """
    æ ¹æ®å®ä½“åç§°å’Œä½ç½®æ¨æ–­æ ‡ç­¾

    Args:
        name: å®ä½“åç§°
        position: 'head' æˆ– 'tail'
        relation: å…³ç³»ç±»å‹
    """
    name_lower = name.lower()

    # ä¼˜å…ˆä½¿ç”¨å…³ç³»æ˜ å°„è¡¨
    if relation in RELATION_LABEL_MAP:
        labels = RELATION_LABEL_MAP[relation]
        return labels[0] if position == "head" else labels[1]

    # æ ¹æ®å¸¸è§ç–¾ç—…åç§°æ¨æ–­
    diseases = ["diabetes", "hypertension", "heart disease", "obesity", "cancer",
                 "asthma", "arthritis", "anemia", "gout", "kidney disease"]
    if any(d in name_lower for d in diseases):
        return "Disease"

    # æ ¹æ®è¥å…»ç´ åç§°æ¨æ–­
    nutrients = ["protein", "carbohydrate", "fat", "fiber", "vitamin",
                  "mineral", "calcium", "iron", "zinc", "sodium", "potassium"]
    if any(n in name_lower for n in nutrients):
        return "Nutrient"

    # é»˜è®¤ä½¿ç”¨é€šç”¨æ ‡ç­¾
    return "Entity"


# ================= æ ¸å¿ƒé€»è¾‘ =================
driver = GraphDatabase.driver(NEO4J_URI(), auth=NEO4J_AUTH())


def create_indexes(session):
    """åˆ›å»ºç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢"""
    indexes = [
        "CREATE FULLTEXT INDEX search_index IF NOT EXISTS FOR (n:Entity) ON EACH [n.name]",
        "CREATE INDEX entity_name_idx IF NOT EXISTS FOR (n:Entity) ON (n.name)",
    ]
    for idx in indexes:
        try:
            session.run(idx)
        except Exception as e:
            print(f"ç´¢å¼•åˆ›å»ºè·³è¿‡æˆ–å¤±è´¥: {e}")


def clear_database(session):
    """æ¸…ç©ºæ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
    confirm = input("è­¦å‘Šï¼šè¿™å°†åˆ é™¤æ‰€æœ‰ç°æœ‰æ•°æ®ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
    if confirm.lower() != 'y':
        print("å·²å–æ¶ˆ")
        return False
    session.run("MATCH (n) DETACH DELETE n")
    print("å·²æ¸…ç©ºæ•°æ®åº“")
    return True


def import_json_triplets(session, json_path):
    """ä»JSONæ–‡ä»¶å¯¼å…¥ä¸‰å…ƒç»„"""
    print(f"ğŸ“„ æ­£åœ¨å¯¼å…¥: {json_path}")

    with open(json_path, 'r', encoding='utf-8') as f:
        triplets = json.load(f)

    if not triplets:
        print(f"  âš ï¸ ç©ºæ–‡ä»¶ï¼Œè·³è¿‡")
        return 0

    count = 0
    for t in triplets:
        head = t.get('head', '').strip()
        relation = t.get('relation', '').strip()
        tail = t.get('tail', '').strip()
        source = t.get('source', '')

        if not head or not relation or not tail:
            continue

        try:
            # ä½¿ç”¨å®é™…å…³ç³»ç±»å‹åç§°ï¼ˆéœ€è¦ç”¨åå¼•å·åŒ…è£¹ï¼‰
            rel_type = relation.replace(' ', '_').replace('-', '_')

            # æ¨æ–­å®ä½“æ ‡ç­¾
            head_label = infer_entity_label(head, "head", relation)
            tail_label = infer_entity_label(tail, "tail", relation)

            # åˆ›å»ºå®ä½“å’Œå…³ç³»
            session.run(f"""
                MERGE (h:{head_label} {{name: $head}})
                MERGE (t:{tail_label} {{name: $tail}})
                MERGE (h)-[r:`{rel_type}` {{source: $source}}]->(t)
            """, head=head, tail=tail, relation=relation, source=source)
            count += 1
        except Exception as e:
            print(f"  âŒ å¯¼å…¥å¤±è´¥: {head} -[{relation}]-> {tail}: {e}")

    print(f"  âœ… æˆåŠŸå¯¼å…¥ {count} æ¡å…³ç³»")
    return count


def import_csv_triplets(session, csv_path):
    """ä»CSVæ–‡ä»¶å¯¼å…¥ä¸‰å…ƒç»„"""
    import pandas as pd

    print(f"ğŸ“„ æ­£åœ¨å¯¼å…¥CSV: {csv_path}")

    df = pd.read_csv(csv_path)
    if df.empty:
        print(f"  âš ï¸ ç©ºæ–‡ä»¶ï¼Œè·³è¿‡")
        return 0

    count = 0
    for _, row in df.iterrows():
        head = str(row.get('head', '')).strip()
        relation = str(row.get('relation', '')).strip()
        tail = str(row.get('tail', '')).strip()
        source = str(row.get('source', '')).strip()

        if not head or not relation or not tail or head == 'nan':
            continue

        try:
            # ä½¿ç”¨å®é™…å…³ç³»ç±»å‹åç§°ï¼ˆéœ€è¦ç”¨åå¼•å·åŒ…è£¹ï¼‰
            rel_type = relation.replace(' ', '_').replace('-', '_')

            # æ¨æ–­å®ä½“æ ‡ç­¾
            head_label = infer_entity_label(head, "head", relation)
            tail_label = infer_entity_label(tail, "tail", relation)

            session.run(f"""
                MERGE (h:{head_label} {{name: $head}})
                MERGE (t:{tail_label} {{name: $tail}})
                MERGE (h)-[r:`{rel_type}` {{source: $source}}]->(t)
            """, head=head, tail=tail, relation=relation, source=source)
            count += 1
        except Exception as e:
            print(f"  âŒ å¯¼å…¥å¤±è´¥: {head} -[{relation}]-> {tail}: {e}")

    print(f"  âœ… æˆåŠŸå¯¼å…¥ {count} æ¡å…³ç³»")
    return count


def import_from_output_history(session):
    """ä» output_history ç›®å½•å¯¼å…¥æ‰€æœ‰æ•°æ®"""
    if not os.path.exists(OUTPUT_HISTORY_DIR):
        print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {OUTPUT_HISTORY_DIR}")
        return 0

    json_files = glob.glob(os.path.join(OUTPUT_HISTORY_DIR, "**/*.json"), recursive=True)
    csv_files = glob.glob(os.path.join(OUTPUT_HISTORY_DIR, "**/*.csv"), recursive=True)

    total = 0
    for json_file in json_files:
        total += import_json_triplets(session, json_file)

    for csv_file in csv_files:
        total += import_csv_triplets(session, csv_file)

    return total


def import_from_directory(session, directory):
    """ä»æŒ‡å®šç›®å½•å¯¼å…¥æ–‡æ¡£ç›´æ¥è§£æï¼ˆéœ€è¦å…ˆè¿è¡ŒLLMæå–ï¼‰"""
    # å¦‚æœç›®å½•ä¸‹æœ‰å·²æå–çš„ä¸‰å…ƒç»„æ–‡ä»¶
    json_files = glob.glob(os.path.join(directory, "*.json"))
    csv_files = glob.glob(os.path.join(directory, "*.csv"))

    total = 0
    for json_file in json_files:
        total += import_json_triplets(session, json_file)

    for csv_file in csv_files:
        total += import_csv_triplets(session, csv_file)

    return total


def show_stats(session):
    """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")

    # å®ä½“æ•°é‡ (æ‰€æœ‰æ ‡ç­¾)
    result = session.run("MATCH (n) RETURN count(n) as count")
    entity_count = result.single()["count"]
    print(f"  å®ä½“æ•°é‡: {entity_count}")

    # å®ä½“æ ‡ç­¾åˆ†å¸ƒ
    print("  å®ä½“æ ‡ç­¾åˆ†å¸ƒ:")
    result = session.run("""
        MATCH (n)
        RETURN labels(n) as labels, count(n) as count
        ORDER BY count DESC
    """)
    for record in result:
        print(f"    {record['labels']}: {record['count']}")

    # å…³ç³»æ•°é‡
    result = session.run("MATCH ()-[r]->() RETURN count(r) as count")
    rel_count = result.single()["count"]
    print(f"  å…³ç³»æ•°é‡: {rel_count}")

    # å…³ç³»ç±»å‹åˆ†å¸ƒ
    print("  å…³ç³»ç±»å‹åˆ†å¸ƒ:")
    result = session.run("""
        MATCH ()-[r]->()
        RETURN r.type as type, count(r) as count
        ORDER BY count DESC
    """)
    for record in result:
        print(f"    {record['type']}: {record['count']}")


def main():
    print("=" * 50)
    print("Neo4j çŸ¥è¯†å›¾è°±å¯¼å…¥å·¥å…·")
    print("=" * 50)

    with driver.session() as session:
        # é€‰é¡¹èœå•
        print("\né€‰æ‹©å¯¼å…¥æ¨¡å¼:")
        print("1. ä» output_history å¯¼å…¥ (build_kg_deepseek.py çš„è¾“å‡º)")
        print("2. ä»æŒ‡å®šç›®å½•å¯¼å…¥")
        print("3. æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡")
        print("4. æ¸…ç©ºæ•°æ®åº“å¹¶é€€å‡º")

        choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()

        if choice == "1":
            create_indexes(session)
            total = import_from_output_history(session)
            print(f"\nğŸ‰ æ€»è®¡å¯¼å…¥ {total} æ¡å…³ç³»")
            show_stats(session)

        elif choice == "2":
            directory = input("è¯·è¾“å…¥ç›®å½•è·¯å¾„: ").strip()
            if os.path.exists(directory):
                create_indexes(session)
                total = import_from_directory(session, directory)
                print(f"\nğŸ‰ æ€»è®¡å¯¼å…¥ {total} æ¡å…³ç³»")
                show_stats(session)
            else:
                print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")

        elif choice == "3":
            show_stats(session)

        elif choice == "4":
            if clear_database(session):
                print("æ•°æ®åº“å·²æ¸…ç©º")

        else:
            print("æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    main()
